import math
from collections import deque
from typing import Callable

import einops
import numpy as np
import torch
import torch.nn.functional as F  # noqa: N812
import torchvision
from diffusers.schedulers.scheduling_ddim import DDIMScheduler
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from transformers import CLIPTextModel, CLIPTokenizer, AutoModel
from torch import Tensor, nn

from lerobot.constants import OBS_ENV_STATE, OBS_ROBOT, ACTION
from lerobot.policies.dit_mt.configuration_dit_mt import DiTMTConfig
from lerobot.policies.normalize import Normalize, Unnormalize
from lerobot.policies.pretrained import PreTrainedPolicy
from lerobot.policies.utils import (
    get_device_from_parameters,
    get_dtype_from_parameters,
    get_output_shape,
    populate_queues,
)

# handling language encoder as a external module
language_encoder = None
pretrained_rgb_encoder = None

class DiTMTPolicy(PreTrainedPolicy):
    """
    DiT Policy as per "The Ingredients for Robotic Diffusion Transformers"
    (paper: https://arxiv.org/abs/2410.10088, code: https://github.com/sudeepdasari/dit-policy)
    """

    config_class = DiTMTConfig
    name = "dit_mt"

    def __init__(
        self,
        config: DiTMTConfig,
        dataset_stats: dict[str, dict[str, Tensor]] | None = None,
    ):
        """
        Args:
            config: Policy configuration class instance or None, in which case the default instantiation of
                the configuration class is used.
            dataset_stats: Dataset statistics to be used for normalization. If not passed here, it is expected
                that they will be passed with a call to `load_state_dict` before the policy is used.
        """
        super().__init__(config)
        config.validate_features()
        self.config = config

        self.normalize_inputs = Normalize(config.input_features, config.normalization_mapping, dataset_stats)
        self.normalize_targets = Normalize(
            config.output_features, config.normalization_mapping, dataset_stats
        )
        self.unnormalize_outputs = Unnormalize(
            config.output_features, config.normalization_mapping, dataset_stats
        )

        # queues are populated during rollout of the policy, they contain the n latest observations and actions
        self._queues = None

        self.model = DiT(config)

        self.reset()

    def get_optim_params(self) -> dict:
        return self.model.parameters()

    def reset(self):
        """Clear observation and action queues. Should be called on `env.reset()`"""
        self._queues = {
            "observation.state": deque(maxlen=self.config.n_obs_steps),
            "action": deque(maxlen=self.config.n_action_steps),
        }
        if self.config.image_features:
            self._queues["observation.images"] = deque(maxlen=self.config.n_obs_steps)
        if self.config.env_state_feature:
            self._queues["observation.environment_state"] = deque(maxlen=self.config.n_obs_steps)

    @torch.no_grad()
    def predict_action_chunk(self, batch: dict[str, torch.Tensor]) -> torch.Tensor:
        """Predict a chunk of actions given environment observations."""
        # stack n latest observations from the queue
        for key in batch:
            if key in self._queues:
                batch[key] = torch.stack(list(self._queues[key]), dim=1)

        # batch = {k: torch.stack(list(self._queues[k]), dim=1) for k in batch if k in self._queues}
        actions = self.model.generate_actions(batch)

        # TODO(rcadene): make above methods return output dictionary?
        actions = self.unnormalize_outputs({ACTION: actions})[ACTION]

        return actions

    @torch.no_grad
    def select_action(self, batch: dict[str, Tensor]) -> Tensor:
        """Select a single action given environment observations.

        This method handles caching a history of observations and an action trajectory generated by the
        underlying diffusion model. Here's how it works:
          - `n_obs_steps` steps worth of observations are cached (for the first steps, the observation is
            copied `n_obs_steps` times to fill the cache).
          - The diffusion model generates `horizon` steps worth of actions.
          - `n_action_steps` worth of actions are actually kept for execution, starting from the current step.
        Schematically this looks like:
            ----------------------------------------------------------------------------------------------
            (legend: o = n_obs_steps, h = horizon, a = n_action_steps)
            |timestep            | n-o+1 | n-o+2 | ..... | n     | ..... | n+a-1 | n+a   | ..... | n-o+h |
            |observation is used | YES   | YES   | YES   | YES   | NO    | NO    | NO    | NO    | NO    |
            |action is generated | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   | YES   |
            |action is used      | NO    | NO    | NO    | YES   | YES   | YES   | NO    | NO    | NO    |
            ----------------------------------------------------------------------------------------------
        Note that this means we require: `n_action_steps <= horizon - n_obs_steps + 1`. Also, note that
        "horizon" may not the best name to describe what the variable actually means, because this period is
        actually measured from the first observation which (if `n_obs_steps` > 1) happened in the past.
        """
        batch = self.normalize_inputs(batch)
        if self.config.image_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch["observation.images"] = torch.stack(
                [batch[key] for key in self.config.image_features], dim=-4
            )
        # Note: It's important that this happens after stacking the images into a single key.
        self._queues = populate_queues(self._queues, batch)

        if len(self._queues["action"]) == 0:
            actions = self.predict_action_chunk(batch)

            self._queues[ACTION].extend(actions.transpose(0, 1))

        action = self._queues[ACTION].popleft()
        return action

    def forward(self, batch: dict[str, Tensor]) -> tuple[Tensor, None]:
        """Run the batch through the model and compute the loss for training or validation."""
        batch = self.normalize_inputs(batch)
        if self.config.image_features:
            batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original
            batch["observation.images"] = torch.stack(
                [batch[key] for key in self.config.image_features], dim=-4
            )
        batch = self.normalize_targets(batch)
        loss = self.model.compute_loss(batch)
        # no output_dict so returning None
        return loss, None

def _make_noise_scheduler(name: str, **kwargs: dict) -> DDPMScheduler | DDIMScheduler:
    """
    Factory for noise scheduler instances of the requested type. All kwargs are passed
    to the scheduler.
    """
    if name == "DDPM":
        return DDPMScheduler(**kwargs)
    elif name == "DDIM":
        return DDIMScheduler(**kwargs)
    else:
        raise ValueError(f"Unsupported noise scheduler type {name}")
    
class DiT(nn.Module):
    def __init__(self, config: DiTMTConfig):
        super().__init__()

        # global language_encoder, pretrained_rgb_encoder

        self.config = config

        # Build observation encoders (depending on which observations are provided).
        if self.config.robot_state_feature:
            robot_state_dim = self.config.robot_state_feature.shape[0]
            self.robot_state_encoder = nn.Sequential(
                nn.Dropout(p=0.2), nn.Linear(robot_state_dim, config.dim_model)
            )
        if self.config.image_features:
            num_images = len(self.config.image_features)

            if config.use_pretrained_dino:
                self.pretrained_rgb_encoder = DINOv2Encoder(config)

                self.rgb_embedding_projection = nn.Linear(self.pretrained_rgb_encoder.hidden_size, config.dim_model)
            else:
                # inner ModuleList: create rgb encoders for each camera
                # outer ModuleList: create rgb encoders groups for each task
                encoders = nn.ModuleList([
                    nn.ModuleList([DiTRgbEncoder(config) for _ in range(num_images)])
                    for _ in range(config.num_learned_tasks)
                ])
            
                self.rgb_encoder = encoders
        if self.config.env_state_feature:
            env_state_dim = self.config.env_state_feature.shape[0]
            self.env_state_encoder = nn.Sequential(
                nn.Dropout(p=0.2), nn.Linear(env_state_dim, config.dim_model)
            )

        self.language_encoder = LanguageEncoder(config).to(self.config.device)

        # Add projection layer
        self.language_embedding_projection = nn.Linear(self.language_encoder.hidden_size, config.dim_model)

        self.noise_net = DiTNoiseNet(self.config)

        self.noise_scheduler = _make_noise_scheduler(
            config.noise_scheduler_type,
            num_train_timesteps=config.num_train_timesteps,
            beta_start=config.beta_start,
            beta_end=config.beta_end,
            beta_schedule=config.beta_schedule,
            clip_sample=config.clip_sample,
            prediction_type=config.prediction_type,
        )

        if config.num_inference_steps is None:
            self.num_inference_steps = self.noise_scheduler.config.num_train_timesteps
        else:
            self.num_inference_steps = config.num_inference_steps

    # ========= inference  ============
    def conditional_sample(
        self, batch_size: int, global_cond: Tensor | None = None, generator: torch.Generator | None = None
    ) -> Tensor:
        device = get_device_from_parameters(self)
        dtype = get_dtype_from_parameters(self)

        # Sample prior.
        sample = torch.randn(
            size=(batch_size, self.config.horizon, self.config.action_feature.shape[0]),
            dtype=dtype,
            device=device,
            generator=generator,
        )

        self.noise_scheduler.set_timesteps(self.num_inference_steps)

        enc_cache = self.noise_net.forward_enc(global_cond)

        for t in self.noise_scheduler.timesteps:
            # Predict model output.
            model_output = self.noise_net.forward_dec(
                sample, 
                torch.full(sample.shape[:1], t, dtype=torch.long, device=sample.device),
                enc_cache
            )
            # Compute previous image: x_t -> x_t-1
            sample = self.noise_scheduler.step(model_output, t, sample, generator=generator).prev_sample

        return sample

    def _prepare_global_conditioning(self, batch: dict[str, Tensor]) -> Tensor:
        """Encode robot state features and image features, and concatenate them all together along with the state vector."""
        # global language_encoder, pretrained_rgb_encoder
        
        batch_size, n_obs_steps = batch[OBS_ROBOT].shape[:2]
        global_cond_feats = []

        # encode text description
        with torch.no_grad():
            language_embedding = self.language_encoder(batch["task"])

        language_cond_feats = self.language_embedding_projection(language_embedding)
        # language embedding as the first token
        # unsqueeze to reshape as (B, 1, D)
        global_cond_feats.append(language_cond_feats.unsqueeze(1))

        # Extract image features.
        if self.config.image_features:

            if self.config.use_pretrained_dino:
                images = einops.rearrange(batch["observation.images"], "b s n ... -> (b s n) ...", b=batch_size, s=n_obs_steps, n=len(self.config.image_features))
                with torch.no_grad():
                    img_cls_tokens = self.pretrained_rgb_encoder(images)
                img_embeddings = self.rgb_embedding_projection(img_cls_tokens)
                img_features = einops.rearrange(
                    img_embeddings, "(b s n) ... -> b (s n) ...", b=batch_size, s=n_obs_steps, n=len(self.config.image_features)
                )
                global_cond_feats.append(img_features)
            else:
                # Combine batch and sequence dims while rearranging to make the camera index dimension first.
                images_per_camera = einops.rearrange(batch["observation.images"], "b s n ... -> n (b s) ...")
                language_cond_for_film = language_cond_feats.repeat(5, 1)
                img_features_list = torch.cat(
                    [
                        encoder(images, language_cond_for_film)
                        for encoder, images in zip(self.rgb_encoder[self.current_expert_id], images_per_camera, strict=True)
                    ]
                )
                # Separate batch and sequence dims back out. The camera index dim gets absorbed into the
                # feature dim (effectively concatenating the camera features).
                img_features = einops.rearrange(
                    img_features_list, "(n b s) ... -> b (s n) ...", b=batch_size, s=n_obs_steps, n=len(self.config.image_features)
                )

                global_cond_feats.append(img_features)
        # Extract robot state features
        if self.config.robot_state_feature:
            robot_state_features = self.robot_state_encoder(batch[OBS_ROBOT])
            global_cond_feats.append(robot_state_features)
        # Extract env state features
        if self.config.env_state_feature:
            env_state_features = self.env_state_encoder(batch[OBS_ENV_STATE])
            global_cond_feats.append(env_state_features)

        # Concatenate features then flatten to (B, T, dim_model).
        return torch.cat(global_cond_feats, dim=1)

    def generate_actions(self, batch: dict[str, Tensor]) -> Tensor:
        """
        This function expects `batch` to have:
        {
            "observation.state": (B, n_obs_steps, state_dim)

            "observation.images": (B, n_obs_steps, num_cameras, C, H, W)
                AND/OR
            "observation.environment_state": (B, environment_dim)
        }
        """
        batch_size, n_obs_steps = batch["observation.state"].shape[:2]
        assert n_obs_steps == self.config.n_obs_steps

        # Encode image features and concatenate them all together along with the state vector.
        global_cond = self._prepare_global_conditioning(batch)  # (B, global_cond_dim)

        # run sampling
        actions = self.conditional_sample(batch_size, global_cond=global_cond)

        # Extract `n_action_steps` steps worth of actions (from the current observation).
        start = n_obs_steps - 1
        end = start + self.config.n_action_steps
        actions = actions[:, start:end]

        return actions

    def compute_loss(self, batch: dict[str, Tensor]) -> Tensor:
        """
        This function expects `batch` to have (at least):
        {
            "observation.state": (B, n_obs_steps, state_dim)

            "observation.images": (B, n_obs_steps, num_cameras, C, H, W)
                AND/OR
            "observation.environment_state": (B, environment_dim)

            "action": (B, horizon, action_dim)
            "action_is_pad": (B, horizon)
        }
        """
        # Input validation.
        assert set(batch).issuperset({"observation.state", "action", "action_is_pad"})
        assert "observation.images" in batch or "observation.environment_state" in batch
        n_obs_steps = batch["observation.state"].shape[1]
        horizon = batch["action"].shape[1]
        assert horizon == self.config.horizon
        assert n_obs_steps == self.config.n_obs_steps

        # Encode image features and concatenate them all together along with the state vector.
        global_cond = self._prepare_global_conditioning(batch)  # (B, global_cond_dim)

        # Forward diffusion.
        trajectory = batch["action"]
        # Sample noise to add to the trajectory.
        eps = torch.randn(trajectory.shape, device=trajectory.device)
        # Sample a random noising timestep for each item in the batch.
        timesteps = torch.randint(
            low=0,
            high=self.noise_scheduler.config.num_train_timesteps,
            size=(trajectory.shape[0],),
            device=trajectory.device,
        ).long()
        # Add noise to the clean trajectories according to the noise magnitude at each timestep.
        noisy_trajectory = self.noise_scheduler.add_noise(trajectory, eps, timesteps)

        # Run the denoising network (that might denoise the trajectory, or attempt to predict the noise).
        _, pred = self.noise_net(noisy_trajectory, timesteps, global_cond=global_cond)

        # Compute the loss.
        # The target is either the original trajectory, or the noise.
        if self.config.prediction_type == "epsilon":
            target = eps
        elif self.config.prediction_type == "sample":
            target = batch["action"]
        else:
            raise ValueError(f"Unsupported prediction type {self.config.prediction_type}")

        loss = F.mse_loss(pred, target, reduction="none")

        # Mask loss wherever the action is padded with copies (edges of the dataset trajectory).
        if self.config.do_mask_loss_for_padding:
            if "action_is_pad" not in batch:
                raise ValueError(
                    "You need to provide 'action_is_pad' in the batch when "
                    f"{self.config.do_mask_loss_for_padding=}."
                )
            in_episode_bound = ~batch["action_is_pad"]
            loss = loss * in_episode_bound.unsqueeze(-1)

        return loss.mean()


class DiTNoiseNet(nn.Module):
    # global_cond_dim is not used here because in DiT Policy,
    # embedding_dim from all modalities should be the same
    # as config.dim_model, and they will be averaged in the
    # forward step of DiTDecoder
    def __init__(self, config: DiTMTConfig):
        super().__init__()

        self._config = config

        # positional encoding blocks
        self.enc_pos = PositionalEncoding(config.dim_model)
        self.register_parameter(
            "dec_pos",
            nn.Parameter(torch.empty(config.horizon, 1, config.dim_model), requires_grad=False),
        )
        nn.init.xavier_uniform_(self.dec_pos.data)

        # input encoder mlps
        self.time_net = TimeNetwork(config.time_dim, config.dim_model)

        action_dim = config.action_feature.shape[0]

        self.ac_proj = nn.Sequential(
            nn.Linear(action_dim, action_dim),
            nn.GELU(approximate="tanh"),
            nn.Linear(action_dim, config.dim_model),
        )

        # encoder blocks
        self.encoder = DiTEncoder(config)

        # decoder blocks
        self.decoder = DiTDecoder(config)

        # turns predicted tokens into epsilons
        self.eps_out = FinalLayer(config.dim_model, action_dim)

    def forward(self, noise_actions, time, global_cond, enc_cache=None):
        if enc_cache is None:
            enc_cache = self.forward_enc(global_cond)
        return enc_cache, self.forward_dec(noise_actions, time, enc_cache)
    
    def forward_enc(self, global_cond):
        # reshape global condition from (B T dim_model) into (T B dim_model)
        global_cond = einops.rearrange(global_cond, 'B T ... -> T B ...')
        pos = self.enc_pos(global_cond)
        enc_cache = self.encoder(global_cond, pos)
        return enc_cache

    def forward_dec(self, noise_actions, time, enc_cache):
        time_enc = self.time_net(time)
        
        ac_tokens = self.ac_proj(noise_actions)
        # reshape actions embedding from (B T dim_model) into (T B dim_model)
        ac_tokens = einops.rearrange(ac_tokens, 'B T ... -> T B ...')
        dec_in = ac_tokens + self.dec_pos

        # apply decoder
        dec_out = self.decoder(dec_in, time_enc, enc_cache)

        # apply final epsilon prediction layer
        output = self.eps_out(dec_out, time_enc, enc_cache[-1])
        return output

class MLP(nn.Module):
    def __init__(self, config: DiTMTConfig):
        super().__init__()
        self.linear1 = nn.Linear(config.dim_model, config.dim_feedforward)
        self.activation = get_activation_fn(config.feedforward_activation)
        self.dropout = nn.Dropout(config.dropout)
        self.linear2 = nn.Linear(config.dim_feedforward, config.dim_model)

    def forward(self, x):
        x = self.linear1(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x

class DiTEncoder(nn.Module):
    """Convenience module for running multiple encoder layers."""
    def __init__(self, config: DiTMTConfig):
        super().__init__()
        num_layers = config.n_encoder_layers
        self.layers = nn.ModuleList([DiTEncoderLayer(config) for _ in range(num_layers)])
        for layer in self.layers:
            layer.reset_parameters()
    
    def forward(self, x: Tensor, pos_embed: Tensor | None = None) -> Tensor:
        for layer in self.layers:
            x = layer(x, pos_embed=pos_embed)
        return x


class DiTEncoderLayer(nn.Module):
    def __init__(
        self, config: DiTMTConfig
    ):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(config.dim_model, config.n_heads, dropout=config.dropout)

        self.norm1 = nn.LayerNorm(config.dim_model)
        self.norm2 = nn.LayerNorm(config.dim_model)

        self.dropout1 = nn.Dropout(config.dropout)
        self.dropout3 = nn.Dropout(config.dropout)

        self.mlp = MLP(config)

    def forward(self, src, pos_embed):
        q = k = with_pos_embed(src, pos_embed)
        src2, _ = self.self_attn(q, k, value=src, need_weights=False)
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.mlp(src)
        src = src + self.dropout3(src2)
        src = self.norm2(src)
        return src

    def reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)


class ShiftScaleMod(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.act = nn.SiLU()
        self.scale = nn.Linear(dim, dim)
        self.shift = nn.Linear(dim, dim)

    def forward(self, x, c):
        c = self.act(c)
        return x * self.scale(c)[None] + self.shift(c)[None]

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.scale.weight)
        nn.init.xavier_uniform_(self.shift.weight)
        nn.init.zeros_(self.scale.bias)
        nn.init.zeros_(self.shift.bias)


class ZeroScaleMod(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.act = nn.SiLU()
        self.scale = nn.Linear(dim, dim)

    def forward(self, x, c):
        c = self.act(c)
        return x * self.scale(c)[None]

    def reset_parameters(self):
        nn.init.zeros_(self.scale.weight)
        nn.init.zeros_(self.scale.bias)


class DiTDecoderLayer(nn.Module):
    def __init__(self, config: DiTMTConfig):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(config.dim_model, config.n_heads, dropout=config.dropout)

        self.norm1 = nn.LayerNorm(config.dim_model)
        self.norm2 = nn.LayerNorm(config.dim_model)

        self.dropout1 = nn.Dropout(config.dropout)
        self.dropout3 = nn.Dropout(config.dropout)

        # create modulation layers
        self.attn_mod1 = ShiftScaleMod(config.dim_model)
        self.attn_mod2 = ZeroScaleMod(config.dim_model)
        self.mlp_mod1 = ShiftScaleMod(config.dim_model)
        self.mlp_mod2 = ZeroScaleMod(config.dim_model)

        self.mlp = MLP(config)

    def forward(self, x, t, cond):
        # process the conditioning vector first
        cond = torch.mean(cond, axis=0)
        cond = cond + t

        x2 = self.attn_mod1(self.norm1(x), cond)
        x2, _ = self.self_attn(x2, x2, x2, need_weights=False)
        x = self.attn_mod2(self.dropout1(x2), cond) + x

        x2 = self.mlp_mod1(self.norm2(x), cond)

        x2 = self.mlp(x2)

        x2 = self.mlp_mod2(self.dropout3(x2), cond)
        return x + x2

    def reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

        for s in (self.attn_mod1, self.attn_mod2, self.mlp_mod1, self.mlp_mod2):
            s.reset_parameters()


class DiTDecoder(nn.Module):
    """Convenience module for running multiple encoder layers."""
    def __init__(self, config: DiTMTConfig):
        super().__init__()
        num_layers = config.n_encoder_layers
        self.layers = nn.ModuleList([DiTDecoderLayer(config) for _ in range(num_layers)])
        for layer in self.layers:
            layer.reset_parameters()
    
    def forward(self, x: Tensor, time_embed: Tensor | None, encoder_out: Tensor) -> Tensor:
        for layer in self.layers:
            x = layer(x, t=time_embed, cond=encoder_out)
        return x


class SpatialSoftmax(nn.Module):
    """
    Spatial Soft Argmax operation described in "Deep Spatial Autoencoders for Visuomotor Learning" by Finn et al.
    (https://arxiv.org/pdf/1509.06113). A minimal port of the robomimic implementation.

    At a high level, this takes 2D feature maps (from a convnet/ViT) and returns the "center of mass"
    of activations of each channel, i.e., keypoints in the image space for the policy to focus on.

    Example: take feature maps of size (512x10x12). We generate a grid of normalized coordinates (10x12x2):
    -----------------------------------------------------
    | (-1., -1.)   | (-0.82, -1.)   | ... | (1., -1.)   |
    | (-1., -0.78) | (-0.82, -0.78) | ... | (1., -0.78) |
    | ...          | ...            | ... | ...         |
    | (-1., 1.)    | (-0.82, 1.)    | ... | (1., 1.)    |
    -----------------------------------------------------
    This is achieved by applying channel-wise softmax over the activations (512x120) and computing the dot
    product with the coordinates (120x2) to get expected points of maximal activation (512x2).

    The example above results in 512 keypoints (corresponding to the 512 input channels). We can optionally
    provide num_kp != None to control the number of keypoints. This is achieved by a first applying a learnable
    linear mapping (in_channels, H, W) -> (num_kp, H, W).
    """

    def __init__(self, input_shape, num_kp=None):
        """
        Args:
            input_shape (list): (C, H, W) input feature map shape.
            num_kp (int): number of keypoints in output. If None, output will have the same number of channels as input.
        """
        super().__init__()

        assert len(input_shape) == 3
        self._in_c, self._in_h, self._in_w = input_shape

        if num_kp is not None:
            self.nets = torch.nn.Conv2d(self._in_c, num_kp, kernel_size=1)
            self._out_c = num_kp
        else:
            self.nets = None
            self._out_c = self._in_c

        # we could use torch.linspace directly but that seems to behave slightly differently than numpy
        # and causes a small degradation in pc_success of pre-trained models.
        pos_x, pos_y = np.meshgrid(np.linspace(-1.0, 1.0, self._in_w), np.linspace(-1.0, 1.0, self._in_h))
        pos_x = torch.from_numpy(pos_x.reshape(self._in_h * self._in_w, 1)).float()
        pos_y = torch.from_numpy(pos_y.reshape(self._in_h * self._in_w, 1)).float()
        # register as buffer so it's moved to the correct device.
        self.register_buffer("pos_grid", torch.cat([pos_x, pos_y], dim=1))

    def forward(self, features: Tensor) -> Tensor:
        """
        Args:
            features: (B, C, H, W) input feature maps.
        Returns:
            (B, K, 2) image-space coordinates of keypoints.
        """
        if self.nets is not None:
            features = self.nets(features)

        # [B, K, H, W] -> [B * K, H * W] where K is number of keypoints
        features = features.reshape(-1, self._in_h * self._in_w)
        # 2d softmax normalization
        attention = F.softmax(features, dim=-1)
        # [B * K, H * W] x [H * W, 2] -> [B * K, 2] for spatial coordinate mean in x and y dimensions
        expected_xy = attention @ self.pos_grid
        # reshape to [B, K, 2]
        feature_keypoints = expected_xy.view(-1, self._out_c, 2)

        return feature_keypoints


class DiTRgbEncoderSpatialSoftmax(nn.Module):
    """Encodes an RGB image into a 1D feature vector.

    Includes the ability to normalize and crop the image first.
    """

    def __init__(self, config: DiTMTConfig):
        super().__init__()
        # Set up optional preprocessing.
        if config.crop_shape is not None:
            self.do_crop = True
            # Always use center crop for eval
            self.center_crop = torchvision.transforms.CenterCrop(config.crop_shape)
            if config.crop_is_random:
                self.maybe_random_crop = torchvision.transforms.RandomCrop(config.crop_shape)
            else:
                self.maybe_random_crop = self.center_crop
        else:
            self.do_crop = False

        # Set up backbone.
        backbone_model = getattr(torchvision.models, config.vision_backbone)(
            weights=config.pretrained_backbone_weights
        )
        # Note: This assumes that the layer4 feature map is children()[-3]
        # TODO(alexander-soare): Use a safer alternative.
        self.backbone = nn.Sequential(*(list(backbone_model.children())[:-2]))
        if config.use_group_norm:
            if config.pretrained_backbone_weights:
                raise ValueError(
                    "You can't replace BatchNorm in a pretrained model without ruining the weights!"
                )
            self.backbone = _replace_submodules(
                root_module=self.backbone,
                predicate=lambda x: isinstance(x, nn.BatchNorm2d),
                func=lambda x: nn.GroupNorm(num_groups=x.num_features // 16, num_channels=x.num_features),
            )

        # Set up pooling and final layers.
        # Use a dry run to get the feature map shape.
        # The dummy input should take the number of image channels from `config.image_features` and it should
        # use the height and width from `config.crop_shape` if it is provided, otherwise it should use the
        # height and width from `config.image_features`.

        # Note: we have a check in the config class to make sure all images have the same shape.
        images_shape = next(iter(config.image_features.values())).shape
        dummy_shape_h_w = config.crop_shape if config.crop_shape is not None else images_shape[1:]
        dummy_shape = (1, images_shape[0], *dummy_shape_h_w)
        feature_map_shape = get_output_shape(self.backbone, dummy_shape)[1:]

        self.pool = SpatialSoftmax(feature_map_shape, num_kp=config.spatial_softmax_num_keypoints)
        self.feature_dim = config.dim_model
        self.out = nn.Linear(config.spatial_softmax_num_keypoints * 2, self.feature_dim)
        self.relu = nn.ReLU()

    def forward(self, x: Tensor) -> Tensor:
        """
        Args:
            x: (B, C, H, W) image tensor with pixel values in [0, 1].
        Returns:
            (B, D) image feature.
        """
        # Preprocess: maybe crop (if it was set up in the __init__).
        if self.do_crop:
            if self.training:  # noqa: SIM108
                x = self.maybe_random_crop(x)
            else:
                # Always use center crop for eval.
                x = self.center_crop(x)
        # Extract backbone feature.
        x = torch.flatten(self.pool(self.backbone(x)), start_dim=1)
        # Final linear layer with non-linearity.
        x = self.relu(self.out(x))
        return x


def _replace_submodules(
    root_module: nn.Module, predicate: Callable[[nn.Module], bool], func: Callable[[nn.Module], nn.Module]
) -> nn.Module:
    """
    Args:
        root_module: The module for which the submodules need to be replaced
        predicate: Takes a module as an argument and must return True if the that module is to be replaced.
        func: Takes a module as an argument and returns a new module to replace it with.
    Returns:
        The root module with its submodules replaced.
    """
    if predicate(root_module):
        return func(root_module)

    replace_list = [k.split(".") for k, m in root_module.named_modules(remove_duplicate=True) if predicate(m)]
    for *parents, k in replace_list:
        parent_module = root_module
        if len(parents) > 0:
            parent_module = root_module.get_submodule(".".join(parents))
        if isinstance(parent_module, nn.Sequential):
            src_module = parent_module[int(k)]
        else:
            src_module = getattr(parent_module, k)
        tgt_module = func(src_module)
        if isinstance(parent_module, nn.Sequential):
            parent_module[int(k)] = tgt_module
        else:
            setattr(parent_module, k, tgt_module)
    # verify that all BN are replaced
    assert not any(predicate(m) for _, m in root_module.named_modules(remove_duplicate=True))
    return root_module


class DiTRgbEncoder(nn.Module):
    """Encodes an RGB image into a 1D feature vector.

    Includes the ability to normalize and crop the image first.
    """
    def __init__(self, config: DiTMTConfig):
        super().__init__()
        norm_layer = _make_norm(config.vision_backbone_norm_name, config.vision_backbone_norm_num_groups)
        self._size = config.vision_backbone_size
        weights = config.pretrained_backbone_weights
        self._model = _construct_resnet(self._size, norm_layer, weights)
        self._model.fc = nn.Identity()
        self._avg_pool = config.avg_pool
        if not config.avg_pool:
            self._model.avgpool = nn.Identity()

    def forward(self, x):
        if self._avg_pool:
            return self._model(x)
        B = x.shape[0]
        x = self._model(x)
        x = x.reshape((B, self.embed_dim, -1))
        return x.transpose(1, 2)
    
    @property
    def embed_dim(self):
        return {18: 512, 34: 512, 50: 2048}[self._size]


def _make_norm(norm_name, norm_num_groups):
    if norm_name == "batch_norm":
        return nn.BatchNorm2d
    if norm_name == "group_norm":
        num_groups = norm_num_groups
        return lambda num_channels: nn.GroupNorm(num_groups, num_channels)
    if norm_name == "diffusion_policy":

        def _gn_builder(num_channels):
            num_groups = int(num_channels // 16)
            return nn.GroupNorm(num_groups, num_channels)

        return _gn_builder
    raise NotImplementedError(f"Missing norm layer: {norm_name}")

def _construct_resnet(size, norm, weights=None):
    if size == 18:
        w = torchvision.models.ResNet18_Weights
        m = torchvision.models.resnet18(norm_layer=norm)
    elif size == 34:
        w = torchvision.models.ResNet34_Weights
        m = torchvision.models.resnet34(norm_layer=norm)
    elif size == 50:
        w = torchvision.models.ResNet50_Weights
        m = torchvision.models.resnet50(norm_layer=norm)
    else:
        raise NotImplementedError(f"Missing size: {size}")

    if weights is not None:
        w = w.verify(weights).get_state_dict(progress=True)
        if norm is not nn.BatchNorm2d:
            w = {
                k: v
                for k, v in w.items()
                if "running_mean" not in k and "running_var" not in k
            }
        m.load_state_dict(w)
    return m

class DINOv2Encoder(nn.Module):
    def __init__(self, config: DiTMTConfig):
        super().__init__()
        self.config = config
        self._model = AutoModel.from_pretrained('facebook/dinov2-base')
        self._model.to(config.device)
        self._model.requires_grad_(False)
        self._model.eval()

        self.hidden_size = self._model.config.hidden_size

    def forward(self, x: Tensor) -> Tensor:
        outputs = self._model(x)
        cls_token = outputs.pooler_output # (B, 768)

        return cls_token

def get_activation_fn(activation):
    """Return an activation function given a string"""
    if activation == "relu":
        return F.relu
    if activation == "gelu":
        return nn.GELU(approximate="tanh")
    if activation == "glu":
        return F.glu
    raise RuntimeError(f"activation should be relu/gelu/glu, not {activation}.")


def with_pos_embed(tensor, pos=None):
    return tensor if pos is None else tensor + pos

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        # Compute the positional encodings once in log space
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2, dtype=torch.float)
            * -(np.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer("pe", nn.Parameter(pe, requires_grad=False))

    def forward(self, x):
        """
        Args:
            x: Tensor of shape (seq_len, batch_size, d_model)

        Returns:
            Tensor of shape (seq_len, batch_size, d_model) with positional encodings added
        """
        pe = self.pe[:x.shape[0]]
        pe = pe.repeat((1, x.shape[1], 1))
        return pe.detach().clone()

# class PositionalEncoding(nn.Module):

#     def __init__(self, d_model, max_len=5000):
#         super(PositionalEncoding, self).__init__()

#         pe = torch.zeros(max_len, d_model)
#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
#         pe[:, 0::2] = torch.sin(position * div_term)
#         pe[:, 1::2] = torch.cos(position * div_term)
#         pe = pe.unsqueeze(0).transpose(0, 1)
#         self.register_parameter('pe', nn.Parameter(pe, requires_grad=False))

#     def forward(self, x):
#         x = x + self.pe[:x.size(0), :]
#         return x

class TimeNetwork(nn.Module):
    def __init__(self, time_dim, out_dim, learnable_w=False):
        assert time_dim % 2 == 0, "time_dim must be even!"
        half_dim = int(time_dim // 2)
        super().__init__()

        w = np.log(10000) / (half_dim - 1)
        w = torch.exp(torch.arange(half_dim) * -w).float()
        self.register_parameter("w", nn.Parameter(w, requires_grad=learnable_w))

        self.out_net = nn.Sequential(
            nn.Linear(time_dim, out_dim), nn.SiLU(), nn.Linear(out_dim, out_dim)
        )

    def forward(self, x):
        assert len(x.shape) == 1, "assumes 1d input timestep array"
        x = x[:, None] * self.w[None]
        x = torch.cat((torch.cos(x), torch.sin(x)), dim=1)
        return self.out_net(x)


class FinalLayer(nn.Module):
    def __init__(self, hidden_size, out_size):
        super().__init__()
        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)
        self.linear = nn.Linear(hidden_size, out_size, bias=True)
        self.adaLN_modulation = nn.Sequential(
            nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True)
        )

    def forward(self, x, t, cond):
        # process the conditioning vector first
        cond = torch.mean(cond, axis=0)
        cond = cond + t

        shift, scale = self.adaLN_modulation(cond).chunk(2, dim=1)
        x = x * scale[None] + shift[None]
        x = self.linear(x)
        return x.transpose(0, 1)

    def reset_parameters(self):
        for p in self.parameters():
            nn.init.zeros_(p)


class LanguageEncoder(nn.Module):
    """
    Language Encoder using pretrained CLIP "Learning Transferable Visual Models From Natural Language Supervision"
    (paper: https://arxiv.org/pdf/2103.00020)
    """
    
    def __init__(self, config:DiTMTConfig):
        super().__init__()
        
        self.config = config

        self.tokenizer = CLIPTokenizer.from_pretrained(config.language_model_name)
        self.clip_model = CLIPTextModel.from_pretrained(config.language_model_name)
        self.cache = {}
        
        # Get the hidden size of CLIP model
        self.hidden_size = self.clip_model.config.hidden_size
        
        # Freeze the base model if specified
        if config.freeze_language_pretrained:
            self.clip_model.requires_grad_(False)
            # for param in self.clip_model.parameters():
            #     param.requires_grad = False
            self.clip_model.eval()

    def forward(self, texts):
        """
        Encodes input text into embeddings and projects to specified output dimension.
        
        Args:
            texts (list[str]): List of text strings to be encoded (batch size B).
        
        Returns:
            torch.Tensor: The projected text embeddings of shape (B, output_dim).
        """
        # Check cache first
        cached_embeddings = []
        uncached_texts = []
        uncached_indices = []
        
        for i, text in enumerate(texts):
            if text in self.cache:
                cached_embeddings.append(self.cache[text])
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # Process uncached texts
        if uncached_texts:
            # Tokenize the input texts
            inputs = self.tokenizer(
                uncached_texts, 
                padding=True, 
                truncation=True, 
                return_tensors="pt",
                max_length=self.tokenizer.model_max_length
            )
            
            # Move inputs to the same device as the model
            device = self.config.device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # Get embeddings from CLIP text model
            with torch.set_grad_enabled(not self.clip_model.training):
                outputs = self.clip_model(**inputs)
            
            # Get the EOS token embeddings
            uncached_embeddings = outputs.pooler_output
            
            # Update cache
            for text, embedding in zip(uncached_texts, uncached_embeddings):
                self.cache[text] = embedding.detach().cpu()  # Store in CPU to save GPU memory
        
        # Combine cached and uncached embeddings in the original order
        all_embeddings = [None] * len(texts)
        # Process uncached texts
        if uncached_texts:
            for i, emb in zip(uncached_indices, uncached_embeddings):
                all_embeddings[i] = emb
        for i, text in enumerate(texts):
            if text in self.cache and all_embeddings[i] is None:
                # Move cached embedding to same device as model
                all_embeddings[i] = self.cache[text].to(self.config.device)
        
        # Stack all embeddings into a single tensor
        return torch.stack(all_embeddings)

